{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from https://repo1.maven.org/maven2/io/delta/delta-core_2.11/0.4.0/delta-core_2.11-0.4.0.jar\n",
      "Finished download of delta-core_2.11-0.4.0.jar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%AddJar https://repo1.maven.org/maven2/io/delta/delta-core_2.11/0.4.0/delta-core_2.11-0.4.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.delta.DeltaLog\n",
    "import io.delta.tables._\n",
    "import org.apache.spark.sql.functions._ \n",
    "import org.apache.spark.sql.{SaveMode, SparkSession, DataFrame}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Partitioned table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createPartitionedTable: (data: org.apache.spark.sql.DataFrame, tableName: String, Keys: String)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "  def createPartitionedTable(data: DataFrame, tableName: String, Keys: String): Unit = {\n",
    "  data\n",
    "      .write\n",
    "      .partitionBy(Keys)\n",
    "      .format(\"delta\")\n",
    "      .mode(\"overwrite\")\n",
    "      .option(\"mergeSchema\", \"true\")\n",
    "      .save(\"/opt/partitioned_lake/\" + tableName)\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overwrite only the data that matches predicates over partition columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updatePartitionedTable: (data: org.apache.spark.sql.DataFrame, tableName: String, condition: String)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def updatePartitionedTable(data: DataFrame, tableName: String, condition: String): Unit = {\n",
    "  data\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .option(\"replaceWhere\", condition)\n",
    "      .mode(\"overwrite\")\n",
    "      .option(\"mergeSchema\", \"true\")\n",
    "      .save(\"/opt/partitioned_lake/\" + tableName)\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readTable: (tableName: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " def readTable(tableName: String): DataFrame = {\n",
    "   val df = spark\n",
    "      .read\n",
    "      .format(\"delta\")\n",
    "      .load(\"/opt/partitioned_lake/\" + tableName)\n",
    "     df\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sales_df = [id: string, product_id: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: string, product_id: string ... 2 more fields]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sales_df = spark.read.option(\"header\",true).csv(\"Sale_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modifiedDF = [id: string, product_id: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: string, product_id: string ... 3 more fields]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val modifiedDF = sales_df.withColumn(\"date\", date_format($\"created_at\", \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+-----+----------+\n",
      "| id|product_id|         created_at|units|      date|\n",
      "+---+----------+-------------------+-----+----------+\n",
      "|  1|       205|1970-01-01 00:00:15|    1|1970-01-01|\n",
      "|  2|       338|1970-01-01 00:01:15|    2|1970-01-01|\n",
      "|  3|       461|1970-01-01 00:01:50|    5|1970-01-01|\n",
      "|  4|       705|1970-01-06 03:25:55|    2|1970-01-06|\n",
      "|  5|       919|1970-01-06 03:26:15|    2|1970-01-06|\n",
      "|  6|       216|1970-01-06 03:26:35|    2|1970-01-06|\n",
      "|  7|       668|2009-11-14 06:41:05|    5|2009-11-14|\n",
      "|  8|       705|2009-11-14 06:41:10|    2|2009-11-14|\n",
      "|  9|       900|2009-11-14 06:41:15|    5|2009-11-14|\n",
      "| 10|       275|2009-11-14 06:41:20|    1|2009-11-14|\n",
      "| 11|        80|2009-11-14 06:42:10|    1|2009-11-14|\n",
      "| 12|        88|2010-03-26 21:46:45|    1|2010-03-26|\n",
      "| 13|       240|2010-03-26 21:47:00|    5|2010-03-26|\n",
      "| 14|       226|2010-03-26 21:47:20|    5|2010-03-26|\n",
      "| 15|       463|2010-03-26 21:48:00|    3|2010-03-26|\n",
      "| 16|      1014|2010-03-26 21:48:30|    4|2010-03-26|\n",
      "| 17|       900|2010-03-27 20:20:55|    1|2010-03-27|\n",
      "| 18|       283|2010-03-27 20:21:30|    1|2010-03-27|\n",
      "| 19|        42|2010-03-27 20:22:15|    1|2010-03-27|\n",
      "| 20|       902|2012-12-21 06:17:10|    1|2012-12-21|\n",
      "+---+----------+-------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modifiedDF.createOrReplaceTempView(\"sales\")\n",
    "modifiedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "createPartitionedTable(modifiedDF, \"sales\", \"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+----------+-----+\n",
      "| id|product_id|         created_at|      date|units|\n",
      "+---+----------+-------------------+----------+-----+\n",
      "|  1|       205|1970-01-01 00:00:15|1970-01-01| 10.0|\n",
      "|  2|       338|1970-01-01 00:01:15|1970-01-01| 20.0|\n",
      "|  3|       461|1970-01-01 00:01:50|1970-01-01| 50.0|\n",
      "+---+----------+-------------------+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res_df = [id: string, product_id: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: string, product_id: string ... 3 more fields]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val res_df = spark.sql(\"select id, \"+\n",
    "\"product_id,created_at,date ,case when date='1970-01-01' then units*10 else units end as units from sales where date='1970-01-01'\")\n",
    "res_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "updatePartitionedTable(res_df,\"sales\", \"date= '1970-01-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+-----+----------+\n",
      "|id |product_id|created_at         |units|date      |\n",
      "+---+----------+-------------------+-----+----------+\n",
      "|12 |88        |2010-03-26 21:46:45|1    |2010-03-26|\n",
      "|13 |240       |2010-03-26 21:47:00|5    |2010-03-26|\n",
      "|14 |226       |2010-03-26 21:47:20|5    |2010-03-26|\n",
      "|15 |463       |2010-03-26 21:48:00|3    |2010-03-26|\n",
      "|16 |1014      |2010-03-26 21:48:30|4    |2010-03-26|\n",
      "|7  |668       |2009-11-14 06:41:05|5    |2009-11-14|\n",
      "|8  |705       |2009-11-14 06:41:10|2    |2009-11-14|\n",
      "|9  |900       |2009-11-14 06:41:15|5    |2009-11-14|\n",
      "|10 |275       |2009-11-14 06:41:20|1    |2009-11-14|\n",
      "|11 |80        |2009-11-14 06:42:10|1    |2009-11-14|\n",
      "|17 |900       |2010-03-27 20:20:55|1    |2010-03-27|\n",
      "|18 |283       |2010-03-27 20:21:30|1    |2010-03-27|\n",
      "|19 |42        |2010-03-27 20:22:15|1    |2010-03-27|\n",
      "|4  |705       |1970-01-06 03:25:55|2    |1970-01-06|\n",
      "|5  |919       |1970-01-06 03:26:15|2    |1970-01-06|\n",
      "|6  |216       |1970-01-06 03:26:35|2    |1970-01-06|\n",
      "|1  |205       |1970-01-01 00:00:15|10.0 |1970-01-01|\n",
      "|2  |338       |1970-01-01 00:01:15|20.0 |1970-01-01|\n",
      "|3  |461       |1970-01-01 00:01:50|50.0 |1970-01-01|\n",
      "|20 |902       |2012-12-21 06:17:10|1    |2012-12-21|\n",
      "+---+----------+-------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readTable(\"sales\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
